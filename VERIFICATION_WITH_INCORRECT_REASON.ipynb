{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af4eff9-f26d-454a-8be5-2b8a030d3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae914809-5267-4c4c-853e-f5ce5acf8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSON file\n",
    "file_path = 'C:/Users/HP/Desktop/angles_output_085347.json'  # Replace with your file path\n",
    "\n",
    "data = []\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d53708-28da-4ebb-ac06-d7d2d8a98e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON data to DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3942285f-9e37-48da-a7d1-574e081aaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = df.drop('Label', axis=1).values\n",
    "y = df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d288ce36-bdb3-467f-a444-072354921348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb098a9-cde5-4458-89bc-02c7a08c4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b788ba8d-d320-436d-bd76-d4a51598dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in:\n",
      "Training data (X_train): 788\n",
      "Testing data (X_test): 197\n"
     ]
    }
   ],
   "source": [
    "# Shape of each dataset\n",
    "print(\"Number of samples in:\")\n",
    "print(f\"Training data (X_train): {X_train.shape[0]}\")\n",
    "print(f\"Testing data (X_test): {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccf7e4a-2bea-4ec8-a9c4-2069ff80afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83080e0f-f820-40a5-88e4-fd1a97a3f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for LSTM: (samples, timesteps, features)\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92fde3f2-b0b9-4779-a506-39c661fd1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Input(shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(50))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9fdf6b-22f8-4e64-946b-313f3bc0c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39131ef-6801-48b0-8c1e-7bb39a994eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.2998 - loss: 1.5741 - val_accuracy: 0.5178 - val_loss: 1.3495\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5581 - loss: 1.3260 - val_accuracy: 0.6802 - val_loss: 1.2385\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6667 - loss: 1.2262 - val_accuracy: 0.7005 - val_loss: 1.1176\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6954 - loss: 1.1203 - val_accuracy: 0.7259 - val_loss: 1.0532\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7107 - loss: 1.0726 - val_accuracy: 0.7513 - val_loss: 0.9915\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7333 - loss: 1.0068 - val_accuracy: 0.7360 - val_loss: 0.9460\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7490 - loss: 0.9568 - val_accuracy: 0.7868 - val_loss: 0.8953\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7864 - loss: 0.9015 - val_accuracy: 0.8071 - val_loss: 0.8344\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8078 - loss: 0.8250 - val_accuracy: 0.8376 - val_loss: 0.7879\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7903 - loss: 0.8322 - val_accuracy: 0.8020 - val_loss: 0.7647\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8173 - loss: 0.7638 - val_accuracy: 0.8477 - val_loss: 0.7230\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7888 - loss: 0.7920 - val_accuracy: 0.8426 - val_loss: 0.6955\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8078 - loss: 0.7158 - val_accuracy: 0.8477 - val_loss: 0.6733\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8214 - loss: 0.7178 - val_accuracy: 0.8680 - val_loss: 0.6461\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8432 - loss: 0.6661 - val_accuracy: 0.8934 - val_loss: 0.6207\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8537 - loss: 0.6628 - val_accuracy: 0.8832 - val_loss: 0.5967\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.6519 - val_accuracy: 0.8883 - val_loss: 0.5782\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8701 - loss: 0.5954 - val_accuracy: 0.8832 - val_loss: 0.5661\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8802 - loss: 0.5740 - val_accuracy: 0.8934 - val_loss: 0.5456\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8780 - loss: 0.5547 - val_accuracy: 0.9188 - val_loss: 0.5331\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8769 - loss: 0.5432 - val_accuracy: 0.8934 - val_loss: 0.5178\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8686 - loss: 0.5506 - val_accuracy: 0.9036 - val_loss: 0.4981\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8743 - loss: 0.5388 - val_accuracy: 0.9086 - val_loss: 0.4857\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8808 - loss: 0.5166 - val_accuracy: 0.8883 - val_loss: 0.4839\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8791 - loss: 0.5214 - val_accuracy: 0.8985 - val_loss: 0.4654\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9004 - loss: 0.4811 - val_accuracy: 0.9188 - val_loss: 0.4551\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9008 - loss: 0.4665 - val_accuracy: 0.9340 - val_loss: 0.4332\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8898 - loss: 0.4619 - val_accuracy: 0.9188 - val_loss: 0.4101\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9088 - loss: 0.4709 - val_accuracy: 0.9188 - val_loss: 0.4029\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9125 - loss: 0.4388 - val_accuracy: 0.9239 - val_loss: 0.4027\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9036 - loss: 0.4194 - val_accuracy: 0.9137 - val_loss: 0.4018\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8943 - loss: 0.4348 - val_accuracy: 0.9289 - val_loss: 0.3870\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9179 - loss: 0.3936 - val_accuracy: 0.9188 - val_loss: 0.3741\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9358 - loss: 0.3606 - val_accuracy: 0.9137 - val_loss: 0.3751\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9175 - loss: 0.3823 - val_accuracy: 0.9340 - val_loss: 0.3581\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9181 - loss: 0.3640 - val_accuracy: 0.9289 - val_loss: 0.3494\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9240 - loss: 0.3397 - val_accuracy: 0.9340 - val_loss: 0.3351\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9145 - loss: 0.3710 - val_accuracy: 0.9289 - val_loss: 0.3311\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9048 - loss: 0.3998 - val_accuracy: 0.9188 - val_loss: 0.3259\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9151 - loss: 0.3684 - val_accuracy: 0.9239 - val_loss: 0.3257\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9271 - loss: 0.3408 - val_accuracy: 0.9239 - val_loss: 0.3320\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9081 - loss: 0.3535 - val_accuracy: 0.9188 - val_loss: 0.3175\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9186 - loss: 0.3437 - val_accuracy: 0.9289 - val_loss: 0.3072\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9241 - loss: 0.3215 - val_accuracy: 0.9289 - val_loss: 0.3016\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9211 - loss: 0.3302 - val_accuracy: 0.9391 - val_loss: 0.3034\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9241 - loss: 0.3184 - val_accuracy: 0.9289 - val_loss: 0.3028\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8992 - loss: 0.3634 - val_accuracy: 0.9137 - val_loss: 0.2956\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9282 - loss: 0.3064 - val_accuracy: 0.9391 - val_loss: 0.2929\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9413 - loss: 0.2787 - val_accuracy: 0.9340 - val_loss: 0.2771\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9185 - loss: 0.3120 - val_accuracy: 0.9289 - val_loss: 0.2780\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_history = lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_lstm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9fab8f-fc8c-4d8f-bb30-88c08eaee1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the history\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(lstm_history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e4c27d-04a0-4f18-b7ab-9d2cd080352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save LSTM model\n",
    "lstm_model.save('new_verification_lstm_model_hip_incorrect_reason_json.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a79838df-677b-417d-aa1e-a1f67b209477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load LSTM model\n",
    "lstm_model = load_model('new_verification_lstm_model_hip_incorrect_reason_json.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b9c078-287f-4b33-ba7e-8b68211797f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\HP\\anaconda3\\envs\\mediapipe\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step\n",
      "The pose in 'C:/Users/HP/Desktop/test_12.jfif' is predicted to be No, issues found: left_shoulder_angle is out of range (167.87°), right_shoulder_angle is out of range (171.83°).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "# Define acceptable ranges for each angle (adjust these values based on your requirements)\n",
    "ANGLE_THRESHOLDS = {\n",
    "    'left_elbow_angle': (60, 180),\n",
    "    'right_elbow_angle': (60, 180),\n",
    "    'left_shoulder_angle': (30, 160),\n",
    "    'right_shoulder_angle': (30, 160),\n",
    "    'left_knee_angle': (70, 180),\n",
    "    'right_knee_angle': (70, 180),\n",
    "    'left_hip_angle': (50, 180),\n",
    "    'right_hip_angle': (50, 180)\n",
    "}\n",
    "\n",
    "# Function to extract joint coordinates from the image\n",
    "def extract_angles_from_image(image_path):\n",
    "    # Initialize Mediapipe Pose model\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose()\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose estimation\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Extract keypoints\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "        # Get coordinates\n",
    "        left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "        right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "        left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "        right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "        left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "        right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "        left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "        right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "        left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "        right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "        left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "        right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "\n",
    "        # Calculate angles\n",
    "        left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "        right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "        left_shoulder_angle = calculate_angle(left_elbow, left_shoulder, left_hip)\n",
    "        right_shoulder_angle = calculate_angle(right_elbow, right_shoulder, right_hip)\n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "\n",
    "        return {\n",
    "            'left_elbow_angle': left_elbow_angle,\n",
    "            'right_elbow_angle': right_elbow_angle,\n",
    "            'left_shoulder_angle': left_shoulder_angle,\n",
    "            'right_shoulder_angle': right_shoulder_angle,\n",
    "            'left_knee_angle': left_knee_angle,\n",
    "            'right_knee_angle': right_knee_angle,\n",
    "            'left_hip_angle': left_hip_angle,\n",
    "            'right_hip_angle': right_hip_angle\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    angle = math.degrees(math.atan2(c[1] - b[1], c[0] - b[0]) - math.atan2(a[1] - b[1], a[0] - b[0]))\n",
    "    angle = abs(angle)\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "    return angle\n",
    "\n",
    "# Function to predict the correctness of the pose and identify issues\n",
    "def predict_pose_correctness(image_path, model):\n",
    "    angles = extract_angles_from_image(image_path)\n",
    "    if angles is None:\n",
    "        print(\"No pose landmarks detected.\")\n",
    "        return None\n",
    "\n",
    "    angles_array = np.array(list(angles.values())).reshape(1, 1, 8)  # Reshape to (1, 1, 8) for LSTM input\n",
    "    prediction = model.predict(angles_array)\n",
    "    correctness = prediction[0][0] > 0.5  # Binary classification threshold\n",
    "\n",
    "    if correctness:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        incorrect_parts = []\n",
    "        for angle_name, angle_value in angles.items():\n",
    "            min_threshold, max_threshold = ANGLE_THRESHOLDS[angle_name]\n",
    "            if not (min_threshold <= angle_value <= max_threshold):\n",
    "                incorrect_parts.append(f\"{angle_name} is out of range ({angle_value:.2f}°)\")\n",
    "\n",
    "        if incorrect_parts:\n",
    "            return f\"No, issues found: {', '.join(incorrect_parts)}\"\n",
    "        else:\n",
    "            return \"No, but no specific angle issues detected.\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    loaded_model = load_model('new_verification_lstm_model_hip_incorrect_reason_json.h5')\n",
    "    image_path = 'C:/Users/HP/Desktop/test_12.jfif'\n",
    "    result = predict_pose_correctness(image_path, loaded_model)\n",
    "\n",
    "    if result is not None:\n",
    "        print(f\"The pose in '{image_path}' is predicted to be {result}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27cc1e-3d08-45c9-96ae-d8539ed619a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
